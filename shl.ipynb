{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom scipy.stats import pearsonr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:59:03.377601Z","iopub.execute_input":"2025-04-06T16:59:03.377942Z","iopub.status.idle":"2025-04-06T16:59:06.041219Z","shell.execute_reply.started":"2025-04-06T16:59:03.377915Z","shell.execute_reply":"2025-04-06T16:59:06.039962Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# File paths","metadata":{}},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/shl-intern-hiring-assessment/dataset/train.csv'\nTEST_PATH = '/kaggle/input/shl-intern-hiring-assessment/dataset/test.csv'\nTRAIN_AUDIO_DIR = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_train/'\nTEST_AUDIO_DIR = '/kaggle/input/shl-intern-hiring-assessment/dataset/audios_test/'\nSUBMISSION_TEMPLATE = '/kaggle/input/shl-intern-hiring-assessment/dataset/sample_submission.csv'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:59:38.127936Z","iopub.execute_input":"2025-04-06T16:59:38.128246Z","iopub.status.idle":"2025-04-06T16:59:38.132618Z","shell.execute_reply.started":"2025-04-06T16:59:38.128222Z","shell.execute_reply":"2025-04-06T16:59:38.131551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef load_data():\n    \"\"\"Load and return training and test dataframes\"\"\"\n    train_df = pd.read_csv(TRAIN_PATH)\n    test_df = pd.read_csv(TEST_PATH)\n    print(f\"Loaded {len(train_df)} training samples and {len(test_df)} test samples\")\n    return train_df, test_df\n\ndef extract_features(file_path, n_mfcc=40, add_delta=True):\n    \"\"\"\n    Extract audio features using librosa\n    \n    Parameters:\n    - file_path: Path to audio file\n    - n_mfcc: Number of MFCC coefficients to extract\n    - add_delta: Whether to add delta and delta-delta features\n    \n    Returns:\n    - Feature vector\n    \"\"\"\n    try:\n        # Load audio file\n        y, sr = librosa.load(file_path, sr=None)\n        \n        # Extract MFCCs\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n        \n        # Get statistics from MFCCs\n        mfcc_features = np.hstack((\n            np.mean(mfcc.T, axis=0),\n            np.std(mfcc.T, axis=0)\n        ))\n        \n        # Add delta and delta-delta features if requested\n        if add_delta:\n            delta_mfcc = librosa.feature.delta(mfcc)\n            delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n            \n            delta_features = np.hstack((\n                np.mean(delta_mfcc.T, axis=0),\n                np.std(delta_mfcc.T, axis=0)\n            ))\n            \n            delta2_features = np.hstack((\n                np.mean(delta2_mfcc.T, axis=0),\n                np.std(delta2_mfcc.T, axis=0)\n            ))\n            \n            return np.hstack((mfcc_features, delta_features, delta2_features))\n        \n        return mfcc_features\n        \n    except Exception as e:\n        print(f\"Error extracting features from {file_path}: {str(e)}\")\n        # Return zeros in case of error\n        feature_size = n_mfcc * 2 * (3 if add_delta else 1)\n        return np.zeros(feature_size)\n\ndef extract_features_batch(df, audio_dir, verbose=True):\n    \"\"\"Extract features from all audio files in dataframe\"\"\"\n    features = []\n    labels = []\n    \n    total = len(df)\n    for i, (_, row) in enumerate(df.iterrows()):\n        if verbose and i % 50 == 0:\n            print(f\"Processing {i}/{total} files...\")\n            \n        path = os.path.join(audio_dir, row['filename'])\n        feature_vector = extract_features(path)\n        features.append(feature_vector)\n        \n        if 'label' in row:\n            labels.append(row['label'])\n    \n    X = np.array(features)\n    y = np.array(labels) if labels else None\n    \n    if verbose:\n        print(f\"Extracted features shape: {X.shape}\")\n        if y is not None:\n            print(f\"Labels shape: {y.shape}\")\n    \n    return X, y\n\ndef train_model(X_train, y_train, X_val=None, y_val=None):\n    \"\"\"Train and optionally tune the model\"\"\"\n    if X_val is not None and y_val is not None:\n        # If validation data is provided, use GridSearchCV for hyperparameter tuning\n        param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [None, 10, 20, 30],\n            'min_samples_split': [2, 5, 10]\n        }\n        \n        model = GridSearchCV(\n            RandomForestRegressor(random_state=42),\n            param_grid,\n            cv=3,\n            scoring='neg_mean_squared_error',\n            verbose=1,\n            n_jobs=-1\n        )\n        \n        model.fit(X_train, y_train)\n        print(f\"Best parameters: {model.best_params_}\")\n        best_model = model.best_estimator_\n    else:\n        # Otherwise, use default parameters\n        best_model = RandomForestRegressor(\n            n_estimators=200,\n            max_depth=None,\n            min_samples_split=2,\n            random_state=42\n        )\n        best_model.fit(X_train, y_train)\n    \n    return best_model\n\ndef evaluate_model(model, X, y, set_name=\"\"):\n    \"\"\"Evaluate model on provided data\"\"\"\n    y_pred = model.predict(X)\n    \n    mse = mean_squared_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n    pearson_corr, _ = pearsonr(y, y_pred)\n    \n    print(f\"=== {set_name} Evaluation ===\")\n    print(f\"Mean Squared Error: {mse:.4f}\")\n    print(f\"R¬≤ Score: {r2:.4f}\")\n    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n    \n    return mse, r2, pearson_corr, y_pred\n\ndef visualize_predictions(y_true, y_pred, title=\"Actual vs Predicted\"):\n    \"\"\"Create a scatter plot of actual vs predicted values\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.scatter(y_true, y_pred, alpha=0.5)\n    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--')\n    plt.xlabel(\"Actual Grammar Score\")\n    plt.ylabel(\"Predicted Grammar Score\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\ndef feature_importance(model, feature_size):\n    \"\"\"Visualize feature importance\"\"\"\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    \n    plt.figure(figsize=(12, 6))\n    plt.title(\"Feature Importances\")\n    plt.bar(range(feature_size), importances[indices[:feature_size]])\n    plt.xticks(range(feature_size), indices[:feature_size])\n    plt.xlabel(\"Feature Index\")\n    plt.ylabel(\"Importance\")\n    plt.tight_layout()\n    plt.show()\n\ndef main():\n    print(\"üéôÔ∏è SHL Grammar Scoring Engine - Improved Version\")\n    \n    # Step 1: Load data\n    train_df, test_df = load_data()\n    \n    # Step 2: Extract features\n    print(\"\\nüìä Extracting features from training data...\")\n    X_train_full, y_train_full = extract_features_batch(train_df, TRAIN_AUDIO_DIR)\n    \n    # Step 3: Split training data for validation\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_full, y_train_full, test_size=0.2, random_state=42\n    )\n    print(f\"Training set: {X_train.shape[0]} samples\")\n    print(f\"Validation set: {X_val.shape[0]} samples\")\n    \n    # Step 4: Train model\n    print(\"\\nüß† Training model...\")\n    model = train_model(X_train, y_train, X_val, y_val)\n    _, _, _, _ = evaluate_model(model, X_val, y_val, \"Validation\")\n    \n    # Step 6: Retrain on full dataset\n    print(\"\\nüîÑ Retraining on full dataset...\")\n    final_model = train_model(X_train_full, y_train_full)\n    \n    # Step 7: Evaluate on full training set\n    _, _, _, train_preds = evaluate_model(final_model, X_train_full, y_train_full, \"Full Training\")\n    # Step 8: Visualize results\n    visualize_predictions(y_train_full, train_preds, \"Training: Actual vs Predicted\")\n    \n    # Step 9: Show feature importance\n    feature_importance(final_model, 20)  # Show top 20 features\n    \n    # Step 10: Extract features from test data\n    print(\"\\nüìä Extracting features from test data...\")\n    X_test, _ = extract_features_batch(test_df, TEST_AUDIO_DIR)\n    \n    # Step 11: Generate predictions for test set\n    print(\"\\nüîÆ Generating predictions for test set...\")\n    test_predictions = final_model.predict(X_test)\n    \n    # Step 12: Display prediction statistics\n    print(f\"Test prediction stats - Min: {min(test_predictions):.2f}, Max: {max(test_predictions):.2f}\")\n\n     # Step 13: Create histogram of predictions\n    plt.figure(figsize=(10, 6))\n    plt.hist(test_predictions, bins=20, alpha=0.7)\n    plt.title(\"Distribution of Predicted Grammar Scores (Test Set)\")\n    plt.xlabel(\"Predicted Score\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(alpha=0.3)\n    plt.show()\n     # Step 14: Create submission file\n    submission = pd.read_csv(SUBMISSION_TEMPLATE)\n    submission['label'] = test_predictions\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"\\n‚úÖ Final submission saved as 'submission.csv'\")\n    print(submission.head())\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:04:04.767933Z","iopub.execute_input":"2025-04-06T17:04:04.768264Z","iopub.status.idle":"2025-04-06T17:07:30.961042Z","shell.execute_reply.started":"2025-04-06T17:04:04.768241Z","shell.execute_reply":"2025-04-06T17:07:30.9599Z"}},"outputs":[],"execution_count":null}]}
